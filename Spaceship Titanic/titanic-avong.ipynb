{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Goal\nUsing passenger information of *Spaceship Titanic*, predict which passengers were transported to a different dimension \\\nPrediction done using logistic regression","metadata":{}},{"cell_type":"markdown","source":"# Table of Contents\n1. Loading the data\n2. Preprocessing\n3. Defining the model\n4. Model training\n5. Model testing","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-27T08:25:24.905542Z","iopub.execute_input":"2022-06-27T08:25:24.905958Z","iopub.status.idle":"2022-06-27T08:25:33.497184Z","shell.execute_reply.started":"2022-06-27T08:25:24.905928Z","shell.execute_reply":"2022-06-27T08:25:33.496350Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Loading the data","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv('/kaggle/input/spaceship-titanic/train.csv')\ndf_test = pd.read_csv('/kaggle/input/spaceship-titanic/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-27T08:25:51.504067Z","iopub.execute_input":"2022-06-27T08:25:51.505114Z","iopub.status.idle":"2022-06-27T08:25:51.585989Z","shell.execute_reply.started":"2022-06-27T08:25:51.505082Z","shell.execute_reply":"2022-06-27T08:25:51.584987Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"Take a look at the data","metadata":{}},{"cell_type":"code","source":"df_train","metadata":{"execution":{"iopub.status.busy":"2022-06-27T08:25:52.939109Z","iopub.execute_input":"2022-06-27T08:25:52.939405Z","iopub.status.idle":"2022-06-27T08:25:52.981929Z","shell.execute_reply.started":"2022-06-27T08:25:52.939381Z","shell.execute_reply":"2022-06-27T08:25:52.981167Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df_train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T08:25:53.732527Z","iopub.execute_input":"2022-06-27T08:25:53.733078Z","iopub.status.idle":"2022-06-27T08:25:53.743095Z","shell.execute_reply.started":"2022-06-27T08:25:53.733050Z","shell.execute_reply":"2022-06-27T08:25:53.742082Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df_test","metadata":{"execution":{"iopub.status.busy":"2022-06-27T08:25:56.810069Z","iopub.execute_input":"2022-06-27T08:25:56.811143Z","iopub.status.idle":"2022-06-27T08:25:56.834367Z","shell.execute_reply.started":"2022-06-27T08:25:56.811114Z","shell.execute_reply":"2022-06-27T08:25:56.833752Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"df_test.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T08:26:13.130183Z","iopub.execute_input":"2022-06-27T08:26:13.130560Z","iopub.status.idle":"2022-06-27T08:26:13.145380Z","shell.execute_reply.started":"2022-06-27T08:26:13.130531Z","shell.execute_reply":"2022-06-27T08:26:13.143929Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"14 columns, including label 'Transported' \\\nLabel is binary T/F, so will use **logistic regression** to predict status of 'Transported' \n\nNot all columns likely to correlate with label. \\\nName, HomePlanet, Destination very unlikely to correlate \\\nRoomService, FoodCourt, ShoppingMall, Spa, VRDeck questionable correlation\\\nWill want to make a cross correlation matrix to get a sense of the data. Will also need to encode features and normalize ","metadata":{}},{"cell_type":"markdown","source":"# Preprocessing\nNeed to transform much of the data to something machine-readable. \\\nWill do 1-hot encoding and normalization of features\n\n<br>","metadata":{}},{"cell_type":"code","source":"def encodePassengerId(df):\n    '''\n    Encodes PassengerId by pulling the group number, i.e. first 4 digits of PassengerID, and returns as scalar\n    Also normalizes the array of IDs\n    '''\n    my_array = [i[:4] for i in df['PassengerId']]\n    my_array = np.array(my_array, dtype=float)\n    \n    avg = np.average(my_array)\n    std = np.std(my_array)\n    my_array = (my_array-avg)/std\n    \n    df['PassengerId'] = my_array\n    \n    return df\n\ndef encodeHomePlanet(df):\n    '''\n    1-hot encoding of HomePlanet.\n    Will remove column HomePlanet and add a new column for each planet (Earth, Europa, Mars). \n    Entries without a HomePlanet will be assigned value 0 for all columns\n    '''\n    (m, n) = df.shape\n    planets = list(set(df['HomePlanet']))\n\n    planets_to_int = dict((p, i) for i, p in enumerate(planets))\n    int_to_planets = dict((i, p) for i, p in enumerate(planets))\n\n    encoded_planets = [planets_to_int[planet] for planet in df['HomePlanet']]\n\n    one_hot = np.zeros([m, len(planets)])\n    for i in range(m):\n        one_hot[i,encoded_planets[i]] = 1\n\n    start_loc = df.columns.get_loc('HomePlanet')\n    for i in range(len(planets)):\n        if str(planets[i]).lower() != 'nan':\n            df.insert(loc=planets_to_int[planets[i]]+start_loc, column=planets[i], value=one_hot[:,planets_to_int[planets[i]]])\n    df = df.drop('HomePlanet', axis=1)\n\n    return df\n\ndef encodeCryoSleep(df):\n    '''\n    Encodes CryoSleep from True/False to 1/0\n    '''\n    df['CryoSleep'] = [float(a) for a in df['CryoSleep']]\n    df['CryoSleep'].fillna(method='bfill', inplace=True)\n    \n    return df\n\ndef encodeCabin(df):\n    '''\n    Encodes Cabin information from deck/num/side to 1-hot of deck, normalized value of num, and 1-hot of side\n    '''\n    m = df.shape[0]\n    nan = float('NaN')\n\n    decks = [None]*m\n    num = [None]*m\n    sides = [None]*m\n    for i in range(m):\n        if str(df['Cabin'][i]).lower() == 'nan':\n            decks[i] = nan\n            num[i] = nan\n            sides[i] = nan\n        else:\n            decks[i] = df['Cabin'][i][0]\n            num[i] = float(df['Cabin'][i][2:-2])\n            sides[i] = df['Cabin'][i][-1]\n    unique_decks = list(set(decks))\n    unique_sides = list(set(sides))\n\n    # 1-hot encoding for side\n    sides_to_int = dict((s,i) for i,s in enumerate(unique_sides))\n    int_to_sides = dict((i,s) for i,s in enumerate(unique_sides))\n\n    encoded_sides = [sides_to_int[side] for side in sides]\n\n    one_hot = np.zeros([m, len(unique_sides)])\n    for i in range(m):\n        one_hot[i,encoded_sides[i]] = 1\n\n    start_loc = df.columns.get_loc('Cabin')\n    for i in range(len(unique_sides)):\n        if str(unique_sides[i]).lower() != 'nan':\n            df.insert(loc=sides_to_int[unique_sides[i]]+start_loc, column='Side '+str(unique_sides[i]),\n                     value=one_hot[:,sides_to_int[unique_sides[i]]])\n\n    # encode and normalize num\n    avg = np.nanmean(num)\n    std = np.nanstd(num)\n    num = (num-avg)/std\n\n    start_loc = df.columns.get_loc('Cabin')\n    df.insert(loc=start_loc, column='Num', value=num)\n    df['Num'].fillna(0, inplace=True)\n\n    # 1-hot encoding for deck value\n    decks_to_int = dict((d, i) for i, d in enumerate(unique_decks))\n    int_to_decks = dict((i, d) for i, d in enumerate(unique_decks))\n\n    encoded_decks = [decks_to_int[deck] for deck in decks]\n\n    one_hot = np.zeros([m, len(unique_decks)])\n    for i in range(m):\n        one_hot[i,encoded_decks[i]] = 1\n\n    start_loc = df.columns.get_loc('Cabin')\n    for i in range(len(unique_decks)):\n        if str(unique_decks[i]).lower() != 'nan':\n            df.insert(loc=decks_to_int[unique_decks[i]]+start_loc, column='Deck '+str(unique_decks[i]),\n                      value=one_hot[:,decks_to_int[unique_decks[i]]])\n\n    df = df.drop('Cabin', axis=1)\n    return df\n\ndef encodeDestination(df):\n    '''\n    1-hot encoding of Destination.\n    Will remove column Destination and add a new column for each planet ('55 Cancri e', 'PSO J318.5-22', 'TRAPPIST-1e'). \n    Entries without a Destination will be assigned value 0 for all columns\n    '''\n    (m, n) = df.shape\n    planets = list(set(df['Destination']))\n\n    planets_to_int = dict((p, i) for i, p in enumerate(planets))\n    int_to_planets = dict((i, p) for i, p in enumerate(planets))\n\n    encoded_planets = [planets_to_int[planet] for planet in df['Destination']]\n\n    one_hot = np.zeros([m, len(planets)])\n    for i in range(m):\n        one_hot[i,encoded_planets[i]] = 1\n\n    start_loc = df.columns.get_loc('Destination')\n    for i in range(len(planets)):\n        if str(planets[i]).lower() != 'nan':\n            df.insert(loc=planets_to_int[planets[i]]+start_loc, column=planets[i], value=one_hot[:,planets_to_int[planets[i]]])\n    df = df.drop('Destination', axis=1)\n\n    return df\n\ndef encodeAge(df):\n    '''\n    normalizes Age\n    '''\n    age = [float(i) for i in df['Age']]\n\n    avg = np.nanmean(age)\n    std = np.nanstd(age)\n    age = (age-avg)/std\n\n    df['Age'] = age\n    df['Age'].fillna(0, inplace=True)\n    return df\n\ndef encodeVIP(df):\n    '''\n    Encodes VIP from True/False to 1/0\n    '''\n    df['VIP'] = [float(a) for a in df['VIP']]\n    df['VIP'].fillna(method='bfill', inplace=True)\n    \n    return df\n\ndef encodeRoomService(df):\n    '''\n    normalizes RoomService\n    '''\n    data = [float(i) for i in df['RoomService']]\n\n    avg = np.nanmean(data)\n    std = np.nanstd(data)\n    data = (data-avg)/std\n\n    df['RoomService'] = data\n    df['RoomService'].fillna(0, inplace=True)\n    return df\n\ndef encodeFoodCourt(df):\n    '''\n    normalizes FoodCourt\n    '''\n    data = [float(i) for i in df['FoodCourt']]\n\n    avg = np.nanmean(data)\n    std = np.nanstd(data)\n    data = (data-avg)/std\n\n    df['FoodCourt'] = data\n    df['FoodCourt'].fillna(0, inplace=True)\n    return df\n\ndef encodeShoppingMall(df):\n    '''\n    normalizes ShoppingMall\n    '''\n    data = [float(i) for i in df['ShoppingMall']]\n\n    avg = np.nanmean(data)\n    std = np.nanstd(data)\n    data = (data-avg)/std\n\n    df['ShoppingMall'] = data\n    df['ShoppingMall'].fillna(0, inplace=True)\n    return df\n\ndef encodeSpa(df):\n    '''\n    normalizes Spa\n    '''\n    data = [float(i) for i in df['Spa']]\n\n    avg = np.nanmean(data)\n    std = np.nanstd(data)\n    data = (data-avg)/std\n\n    df['Spa'] = data\n    df['Spa'].fillna(0, inplace=True)\n    return df\n\ndef encodeVRDeck(df):\n    '''\n    normalizes VRDeck\n    '''\n    data = [float(i) for i in df['VRDeck']]\n\n    avg = np.nanmean(data)\n    std = np.nanstd(data)\n    data = (data-avg)/std\n\n    df['VRDeck'] = data\n    df['VRDeck'].fillna(0, inplace=True)\n    return df\n\ndef encodeTransported(df):\n    '''\n    Encodes Transported from True/False to 1/0\n    '''\n    df['Transported'] = [float(a) for a in df['Transported']]\n    \n    return df\n\ndef encodeDF(df, test=False):\n    '''\n    Cleans data to be machine readable\n    Uses encodePassengerId, encodeHomePlanet, encodeCryoSleep, encodeCabin, encodeDestination, encodeAge,\n        encodeVIP, encodeRoomService, encodeFoodCourt, encodeShoppingMall, encodeSpa, encodeVRDeck, encodeTransported\n    Drops Name    \n    '''\n    df = encodePassengerId(df)\n    df = encodeHomePlanet(df)\n    df = encodeCryoSleep(df)\n    df = encodeCabin(df)\n    df = encodeDestination(df)\n    df = encodeAge(df)\n    df = encodeVIP(df)\n    df = encodeRoomService(df)\n    df = encodeFoodCourt(df)\n    df = encodeShoppingMall(df)\n    df = encodeSpa(df)\n    df = encodeVRDeck(df)\n    if test==False:\n        df = encodeTransported(df)\n\n    df = df.drop('Name', axis=1)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2022-06-27T08:26:59.822108Z","iopub.execute_input":"2022-06-27T08:26:59.822442Z","iopub.status.idle":"2022-06-27T08:26:59.861854Z","shell.execute_reply.started":"2022-06-27T08:26:59.822417Z","shell.execute_reply":"2022-06-27T08:26:59.860721Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df_train = encodeDF(df_train)\ndf_test = encodeDF(df_test, test=True)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T08:27:12.101762Z","iopub.execute_input":"2022-06-27T08:27:12.102080Z","iopub.status.idle":"2022-06-27T08:27:12.587130Z","shell.execute_reply.started":"2022-06-27T08:27:12.102057Z","shell.execute_reply":"2022-06-27T08:27:12.585423Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df_train.iloc[:,:]","metadata":{"execution":{"iopub.status.busy":"2022-06-27T08:27:16.894469Z","iopub.execute_input":"2022-06-27T08:27:16.894887Z","iopub.status.idle":"2022-06-27T08:27:16.941042Z","shell.execute_reply.started":"2022-06-27T08:27:16.894856Z","shell.execute_reply":"2022-06-27T08:27:16.939905Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"df_train.corr().iloc[-1,:].sort_values()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T08:27:18.958488Z","iopub.execute_input":"2022-06-27T08:27:18.959040Z","iopub.status.idle":"2022-06-27T08:27:18.980079Z","shell.execute_reply.started":"2022-06-27T08:27:18.959004Z","shell.execute_reply":"2022-06-27T08:27:18.979133Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Now dataframe has potentially 26 features to learn from. Will likely need to knockout some of these, but for the time being, let's define a model with 26 inputs.\n\n<br>","metadata":{}},{"cell_type":"code","source":"df_train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T08:27:51.913005Z","iopub.execute_input":"2022-06-27T08:27:51.913311Z","iopub.status.idle":"2022-06-27T08:27:51.922311Z","shell.execute_reply.started":"2022-06-27T08:27:51.913288Z","shell.execute_reply":"2022-06-27T08:27:51.921393Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Defining a model\n\nStarting with fully connected feedforward network with 2 hidden layers and 1 output layer","metadata":{}},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(df_train.iloc[:,:-1],df_train.iloc[:,-1],test_size=0.20,random_state=0)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T08:28:06.710126Z","iopub.execute_input":"2022-06-27T08:28:06.710520Z","iopub.status.idle":"2022-06-27T08:28:06.721762Z","shell.execute_reply.started":"2022-06-27T08:28:06.710490Z","shell.execute_reply":"2022-06-27T08:28:06.720507Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"n = df_train.shape[1] - 1\ninputs = tf.keras.Input(shape=(n,))\nx = tf.keras.layers.Dropout(.2)(inputs)\nx = tf.keras.layers.Dense(26, activation='relu')(inputs)\nx = tf.keras.layers.Dense(13, activation='relu')(x)\n# x = tf.keras.layers.Dense(7, activation='relu')(x)\noutputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n\nmodel = tf.keras.Model(inputs=inputs, outputs=outputs, name='2_26_13')","metadata":{"execution":{"iopub.status.busy":"2022-06-27T08:28:09.815631Z","iopub.execute_input":"2022-06-27T08:28:09.816012Z","iopub.status.idle":"2022-06-27T08:28:11.196230Z","shell.execute_reply.started":"2022-06-27T08:28:09.815982Z","shell.execute_reply":"2022-06-27T08:28:11.194828Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T08:28:12.630766Z","iopub.execute_input":"2022-06-27T08:28:12.631140Z","iopub.status.idle":"2022-06-27T08:28:12.636561Z","shell.execute_reply.started":"2022-06-27T08:28:12.631110Z","shell.execute_reply":"2022-06-27T08:28:12.635885Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"model.compile(\n    loss=tf.keras.losses.BinaryCrossentropy(),\n    optimizer=tf.keras.optimizers.Adam(\n    learning_rate=0.001,\n    beta_1=0.9,\n    beta_2=0.999,\n    epsilon=1e-07,\n    amsgrad=False,\n    name='Adam'\n),\n    metrics=[\"accuracy\"],\n)\n\nhistory = model.fit(x_train, y_train, batch_size=128, epochs=250, validation_split=0.2, verbose=False)\n\nresults = model.evaluate(x_test, y_test, batch_size=128)\nprint(\"test loss, test acc:\", results)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T08:28:23.182138Z","iopub.execute_input":"2022-06-27T08:28:23.182493Z","iopub.status.idle":"2022-06-27T08:29:05.063079Z","shell.execute_reply.started":"2022-06-27T08:28:23.182465Z","shell.execute_reply":"2022-06-27T08:29:05.062208Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,4))\nplt.subplot(1,2,1)\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='val')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend(loc=1)\nplt.subplot(1,2,2)\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='val')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(loc=4)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T08:29:05.064736Z","iopub.execute_input":"2022-06-27T08:29:05.065020Z","iopub.status.idle":"2022-06-27T08:29:05.376968Z","shell.execute_reply.started":"2022-06-27T08:29:05.064995Z","shell.execute_reply":"2022-06-27T08:29:05.375774Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Notes from trying a few iterations : \n\n~75% accuracy using CryoSleep, VRDeck, RoomService, Spa, FoodCourt, ShoppingMall\\\n~72% accuracy using CryoSleep, VRDeck, RoomService, Spa \\\n~76% accuracy dropping columns with less than 0.05 correlation coeff. Less overfitting\\\n~76% accuracy dropping columns with less than 0.025 correlation coeff and 0.2 dropout\\\n~76% accuracy using all columns and 4 layers with 64 nodes each\\\n~78% accuracy using all columns and 0.2 dropout [26, 13, 7, 1]\\\n~80% accuracy using all columns and 0.2 dropout [26,13,1]","metadata":{}},{"cell_type":"code","source":"df_test","metadata":{"execution":{"iopub.status.busy":"2022-06-27T08:33:03.047597Z","iopub.execute_input":"2022-06-27T08:33:03.048515Z","iopub.status.idle":"2022-06-27T08:33:03.078554Z","shell.execute_reply.started":"2022-06-27T08:33:03.048485Z","shell.execute_reply":"2022-06-27T08:33:03.077718Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"df_test.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2022-06-27T08:32:14.814921Z","iopub.execute_input":"2022-06-27T08:32:14.815222Z","iopub.status.idle":"2022-06-27T08:32:14.825130Z","shell.execute_reply.started":"2022-06-27T08:32:14.815199Z","shell.execute_reply":"2022-06-27T08:32:14.823735Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(df_test)\ny_pred = y_pred > 0.5\ny_pred = y_pred.reshape(-1,)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T08:39:56.647090Z","iopub.execute_input":"2022-06-27T08:39:56.647463Z","iopub.status.idle":"2022-06-27T08:39:56.796491Z","shell.execute_reply.started":"2022-06-27T08:39:56.647433Z","shell.execute_reply":"2022-06-27T08:39:56.795502Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv('/kaggle/input/spaceship-titanic/test.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-27T08:40:15.997865Z","iopub.execute_input":"2022-06-27T08:40:15.998184Z","iopub.status.idle":"2022-06-27T08:40:16.015993Z","shell.execute_reply.started":"2022-06-27T08:40:15.998159Z","shell.execute_reply":"2022-06-27T08:40:16.014219Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"df_submit = pd.DataFrame({'PassengerId':df_test['PassengerId'], 'Transported':y_pred})","metadata":{"execution":{"iopub.status.busy":"2022-06-27T08:40:17.016772Z","iopub.execute_input":"2022-06-27T08:40:17.017357Z","iopub.status.idle":"2022-06-27T08:40:17.023088Z","shell.execute_reply.started":"2022-06-27T08:40:17.017331Z","shell.execute_reply":"2022-06-27T08:40:17.022195Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"df_submit.to_csv('titanic_AVong.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-27T08:45:35.540953Z","iopub.execute_input":"2022-06-27T08:45:35.541274Z","iopub.status.idle":"2022-06-27T08:45:35.551994Z","shell.execute_reply.started":"2022-06-27T08:45:35.541251Z","shell.execute_reply":"2022-06-27T08:45:35.550919Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('titanic_AVong.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-27T08:45:36.942818Z","iopub.execute_input":"2022-06-27T08:45:36.943166Z","iopub.status.idle":"2022-06-27T08:45:36.952173Z","shell.execute_reply.started":"2022-06-27T08:45:36.943136Z","shell.execute_reply":"2022-06-27T08:45:36.950907Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2022-06-27T08:45:41.933836Z","iopub.execute_input":"2022-06-27T08:45:41.934196Z","iopub.status.idle":"2022-06-27T08:45:41.947549Z","shell.execute_reply.started":"2022-06-27T08:45:41.934167Z","shell.execute_reply":"2022-06-27T08:45:41.946351Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}